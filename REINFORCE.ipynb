{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MqcemFBv8Vp3"
   },
   "source": [
    "#Â Policy-Gradients with the REINFORCE algorithm\n",
    "\n",
    "**Background**:\n",
    "We will train an agent using the REINFORCE policy-gradient algorithm to learn to balance a pole in the OpenAI gym [Cartpole environment](https://gym.openai.com/envs/CartPole-v0).\n",
    "\n",
    "**Learning objectives**:\n",
    "* Understand the policy-gradient approach to directly training a parameterised policy to maximise expected future rewards.\n",
    "* Understand how the policy-gradient theorem allows us to derive a stochastic gradient estimator on the policy, defined in terms of samples drawn from the very same policy.\n",
    "\n",
    "**What is expected of you**:\n",
    " * Go through the explanation, keeping the above learning objectives in mind.\n",
    " * Fill in the missing code (\"#IMPLEMENT-ME\") and train a model to solve the Cartpole-v0 environment in OpenAI gym (you solve it when reward=200).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BNgazEXTqPMf"
   },
   "source": [
    "# A Simple Policy-Gradient Cartpole Example\n",
    "\n",
    "## Introduction\n",
    "\n",
    "There are many different approaches to training RL agents. All of them summarise the environment based on its current **state**, and try to learn how to move from the current state to the next state in order to maximise the **rewards** that the agent will receive over time.Policy-based methods directly tries to learn what action to take next in order to maximise future rewards. \n",
    "\n",
    "Policy-based methods try to learn a policy $\\pi_\\theta(a | s)$ which outputs a distribution over the possible actions $a$, given the current state $s$ of the environment. The goal is to maximise all expected future rewards, under the learned policy:\n",
    "\n",
    "\\begin{align}\n",
    "    J(\\theta) &= \\sum_{t=0}^{T}    \\left[ \\pi_\\theta(a_t|s_t) R(s_t, a_t) \\right] \\\\\n",
    "                        &= \\mathbb{E}_{a_t \\sim \\pi_\\theta} \\left[ R(s_t, a_t) \\right].\n",
    "\\end{align}\n",
    "\n",
    "Policy-**gradient** methods learn $\\pi_\\theta$ by directly differentiating this objective function in terms of $\\theta$:\n",
    "\n",
    "\\begin{align}\n",
    "    \\nabla_\\theta J(\\theta) = \\sum_{t=0}^{T}    \\left[ \\nabla_\\theta \\pi_\\theta(a_t|s_t) R(s_t, a_t) \\right].\n",
    "    \\end{align}\n",
    "\n",
    "We want to solve this by sampling actions from our policy and trying these out in the environment. However the current formulation doesn't allow that. But rearranging the identity $\\nabla_\\theta \\log \\pi_\\theta = \\frac{\\nabla_\\theta \\pi_\\theta}{\\pi_\\theta}$ gives us the trick $\\nabla_\\theta \\pi_\\theta = \\pi_\\theta \\nabla_\\theta \\log \\pi_\\theta$ which we can substitute into the above to get the **[policy-gradient theorem](http://www.scholarpedia.org/article/Policy_gradient_methods)** (also called **[REINFORCE](http://www-anw.cs.umass.edu/~barto/courses/cs687/williams92simple.pdf)**):\n",
    "\n",
    "\\begin{align}\n",
    " \\nabla_\\theta J(\\theta) &= \\sum_{t=0}^{T}    \\left[ \\pi_\\theta(a_t|s_t) \\nabla_\\theta \\log \\pi_\\theta(a_t|s_t) R(s_t, a_t) \\right] \\\\\n",
    " &= \\mathbb{E}_{a_t \\sim \\pi(a|s_t)} \\left[ \\nabla_\\theta \\log \\pi_\\theta(a_t|s_t) R(s_t, a_t) \\right] && \\triangleright \\textrm{Definition of expections.}\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "**NOTE**: \n",
    "* We have a policy $\\pi_\\theta(a|s)$ which tells the agent which action $a$ to take, given the state $s$, and it is parameterised in terms of parameters $\\theta$.\n",
    "* Our goal $J(\\theta)$ is to maximise the rewards the agent receives over time by **choosing actions from this policy** that lead to high future rewards.\n",
    "* We'll use gradient-based optimisation for solving for $\\theta$. We therefore want the gradient of our objective wrt the policy parameters (**given actions drawn from our policy**): $\\nabla_\\theta J(\\theta)$.\n",
    "* We use a simple trick to rearrange this expression for the gradient into an expectation over actions drawn from our policy as it's being learned.\n",
    "* Since we can now sample actions to take from $a \\sim \\pi_\\theta(a | s)$, we can approximate this gradient using **[Monte-Carlo](https://en.wikipedia.org/wiki/Monte_Carlo_integration)** methods:\n",
    "    \\begin{align}\n",
    "        \\nabla_\\theta J(\\theta) &= \\mathbb{E}_{a_t \\sim \\pi(a|s_t)} \\left[ \\nabla_\\theta \\log \\pi_\\theta(a_t|s_t) R(s_t, a_t) \\right] \\\\ \n",
    "        &\\approx \\frac{1}{k} \\sum_k \\nabla_\\theta \\log \\pi_\\theta(a_k|s_t) R(s_t, a_k)\n",
    "    \\end{align}\n",
    "    \n",
    "* If our agent parameterises its actions-distribution as a softmax, then we already know how to do $\\nabla_\\theta \\log \\pi_\\theta(a_t|s_t)$. It's simply the negative log-likelihood or cross-entropy loss!\n",
    "\n",
    "This algorithm is called **Monte-Carlo REINFORCE**, and is one type of policy-gradient algorithm. Let's use this to solve the Cartpole environment!\n",
    "\n",
    "\n",
    "![REINFORCE](https://github.com/s-mawjee/aims-rl/blob/master/images/reinforce.png?raw=true \"REINFORCE Pseudocode\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pip-packages/gym-0.10.9.tar.gz\n",
    "!pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "height": 34,
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 523,
     "status": "ok",
     "timestamp": 1504539377408,
     "user": {
      "displayName": "Stephan Gouws",
      "photoUrl": "//lh4.googleusercontent.com/-6znVyM1oxdg/AAAAAAAAAAI/AAAAAAAAABI/vEPo2Ce7Rpc/s50-c-k-no/photo.jpg",
      "userId": "102606466886131565871"
     },
     "user_tz": -60
    },
    "id": "hJ3ADk9LqZ3p",
    "outputId": "d8aa38c8-d7a1-43e0-94c0-8f624e0675b2"
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "import gym\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "torch.manual_seed(0) # set random seed\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical \n",
    "\n",
    "# for auto-reloading external modules\n",
    "# (if you're curious, see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython)\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch - Use CUDA GPU if available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_matplotlib():\n",
    "    %matplotlib inline\n",
    "    plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "    plt.rcParams['image.interpolation'] = 'nearest'\n",
    "    plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "reset_matplotlib()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "env.seed(0)\n",
    "\n",
    "# IMPLEMENT-ME\n",
    "# Print out observation space for gym environment\n",
    "# Print out action space for gym environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oty9i6N-uo5b"
   },
   "source": [
    "## The Policy-based Agent\n",
    "\n",
    "We begin by parameterising the policy $\\pi_\\theta(a | s)$ as a simple neural network which takes the state (a vector of 4 elements provided by `gym`) as input, and produces a softmax over the possible actions as output. Simple enough. Refer to [torch.nn](https://pytorch.org/docs/stable/nn.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the Architecture of the Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Policy(nn.Module):\n",
    "    def __init__(self, s_size=4, h_size=16, a_size=2):\n",
    "        super(Policy, self).__init__()\n",
    "        # IMPLEMENT-ME\n",
    "        # Define neural network layers. Refer to nn.Linear (https://pytorch.org/docs/stable/nn.html#torch.nn.Linear)\n",
    "        # Two Linear layers\n",
    "\n",
    "    def forward(self, x):\n",
    "        # IMPLEMENT-ME\n",
    "        # Define neural network forward pass, \n",
    "        # with RELU function after first later \n",
    "        # and use softmax on last layer. Refer to torch.nn.functional.softmax (https://pytorch.org/docs/0.3.1/nn.html#torch.nn.functional.softmax)\n",
    "        return pass\n",
    "    \n",
    "    def act(self, state):\n",
    "        # Use policy model to pick an action\n",
    "        state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
    "        probs = self.forward(state).cpu()\n",
    "        m = Categorical(probs)\n",
    "        action = m.sample()\n",
    "        return action.item(), m.log_prob(action)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the Agent with REINFORCE\n",
    "\n",
    "Next we will write the code to update the model parameters given a reward. For this, we we need to sample an action from the model (policy), try it out in the environment, receive the reward from the environment, and use these together to get an estimate of the gradient wrt the policy. We then update the policy to do better the next time using [Adam](https://pytorch.org/docs/stable/optim.html#torch.optim.Adam)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-2\n",
    "number_episodes=1000\n",
    "gamma = 1.0\n",
    "max_episode_length = 1000\n",
    "\n",
    "policy = Policy().to(device)\n",
    "optimizer = optim.Adam(policy.parameters(), lr=learning_rate)\n",
    "\n",
    "def reinforce(n_episodes=number_episodes, max_episode_length=max_episode_length, gamma=gamma, print_every=100):\n",
    "    scores_deque = deque(maxlen=100)\n",
    "    scores = []\n",
    "    for i_episode in range(1, n_episodes+1):\n",
    "        saved_log_probs = []\n",
    "        rewards = []\n",
    "        state = env.reset()\n",
    "        \n",
    "        # IMPLEMENT-ME\n",
    "        # Generate an episodes \n",
    "        for t in range(max_episode_length):\n",
    "            # Pick action and save log_prob using the policy\n",
    "            # Act on env using the action to get the next_state and reward \n",
    "            rewards.append(reward)\n",
    "            saved_log_probs.append(log_prob)\n",
    "            if done:\n",
    "                break \n",
    "        scores_deque.append(sum(rewards))\n",
    "        scores.append(sum(rewards))\n",
    "        \n",
    "        # IMPLEMENT-ME\n",
    "        # Compute G \n",
    "        \n",
    "        # IMPLEMENT-ME\n",
    "        policy_loss = []\n",
    "        for log_prob in saved_log_probs:\n",
    "            # Compute `policy_loss` for each step, using G and log_prob \n",
    "            policy_loss = policy_loss * -1\n",
    "        # Sum loss for episode\n",
    "        policy_loss = torch.cat(policy_loss).sum()\n",
    "        \n",
    "        # Backprop using PyTorch\n",
    "        optimizer.zero_grad()\n",
    "        policy_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if i_episode % print_every == 0:\n",
    "            print('Episode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_deque)))\n",
    "        \n",
    "        if np.mean(scores_deque)>=195.0:\n",
    "            print('Environment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(i_episode-100, np.mean(scores_deque)))\n",
    "            break\n",
    "        \n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = reinforce()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(1, len(scores)+1), scores)\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Watch Trained Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = env.reset()\n",
    "for t in range(2000):\n",
    "    action, _ = policy.act(state)\n",
    "    env.render()\n",
    "    state, reward, done, _ = env.step(action)\n",
    "    if done:\n",
    "        break "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8hxIDITWFseV"
   },
   "source": [
    "# Reference\n",
    "* Deep Learning Indaba: https://github.com/deep-learning-indaba\n",
    "* Medium Blog: https://medium.com/@awjuliani/super-simple-reinforcement-learning-tutorial-part-2-ded33892c724\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "last_runtime": {
    "build_target": "",
    "kind": "local"
   },
   "name": "Practical 6: Deep RL",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
